{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_data(data):\n",
    "    new = np.unique(data, axis=1)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect a list of all headers in each excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "for item in glb.glob(\"G:\\Google\\Electronic Design\\python_projects\\Insight of Ecommerce Data/2020\\*\"):\n",
    "    data = pd.read_excel(item,\n",
    "                         sheet_name=\"orders\",\n",
    "                         header=None,\n",
    "                         index_col=None)\n",
    "    data_np = data.to_numpy()\n",
    "    header.append(data_np[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just make sure that the extracted labels are mutual among all files to avoid wrong attribiutes enter to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_headers = np.intersect1d(header[0], header[1])\n",
    "for i in range(len(header)):\n",
    "    mutual_headers = np.intersect1d(mutual_headers, header[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract all info below the headers from all files and create a new file with all extracted data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the data for \"Area\" header\n",
      "Date Collection of \"Area\" is finished\n",
      "Extracting the data for \"Bundle Deal Indicator\" header\n",
      "Date Collection of \"Bundle Deal Indicator\" is finished\n",
      "Extracting the data for \"Buyer Paid Shipping Fee\" header\n",
      "Date Collection of \"Buyer Paid Shipping Fee\" is finished\n",
      "Extracting the data for \"Cancel reason\" header\n",
      "Date Collection of \"Cancel reason\" is finished\n",
      "Extracting the data for \"Commission Fee\" header\n",
      "Date Collection of \"Commission Fee\" is finished\n",
      "Extracting the data for \"Country\" header\n",
      "Date Collection of \"Country\" is finished\n",
      "Extracting the data for \"Credit Card Discount Total\" header\n",
      "Date Collection of \"Credit Card Discount Total\" is finished\n",
      "Extracting the data for \"Deal Price\" header\n",
      "Date Collection of \"Deal Price\" is finished\n",
      "Extracting the data for \"Delivery Address\" header\n",
      "Date Collection of \"Delivery Address\" is finished\n",
      "Extracting the data for \"District\" header\n",
      "Date Collection of \"District\" is finished\n",
      "Extracting the data for \"Estimated Ship Out Date\" header\n",
      "Date Collection of \"Estimated Ship Out Date\" is finished\n",
      "Extracting the data for \"Estimated Shipping Fee\" header\n",
      "Date Collection of \"Estimated Shipping Fee\" is finished\n",
      "Extracting the data for \"Grand Total\" header\n",
      "Date Collection of \"Grand Total\" is finished\n",
      "Extracting the data for \"No of product in order\" header\n",
      "Date Collection of \"No of product in order\" is finished\n",
      "Extracting the data for \"Note\" header\n",
      "Date Collection of \"Note\" is finished\n",
      "Extracting the data for \"Order Complete Time\" header\n",
      "Date Collection of \"Order Complete Time\" is finished\n",
      "Extracting the data for \"Order Creation Date\" header\n",
      "Date Collection of \"Order Creation Date\" is finished\n",
      "Extracting the data for \"Order ID\" header\n",
      "Date Collection of \"Order ID\" is finished\n",
      "Extracting the data for \"Order Paid Time\" header\n",
      "Date Collection of \"Order Paid Time\" is finished\n",
      "Extracting the data for \"Order Status\" header\n",
      "Date Collection of \"Order Status\" is finished\n",
      "Extracting the data for \"Order Total Weight\" header\n",
      "Date Collection of \"Order Total Weight\" is finished\n",
      "Extracting the data for \"Original Price\" header\n",
      "Date Collection of \"Original Price\" is finished\n",
      "Extracting the data for \"Parent SKU Reference No.\" header\n",
      "Date Collection of \"Parent SKU Reference No.\" is finished\n",
      "Extracting the data for \"Phone Number\" header\n",
      "Date Collection of \"Phone Number\" is finished\n",
      "Extracting the data for \"Product Subtotal\" header\n",
      "Date Collection of \"Product Subtotal\" is finished\n",
      "Extracting the data for \"Quantity\" header\n",
      "Date Collection of \"Quantity\" is finished\n",
      "Extracting the data for \"Receiver Name\" header\n",
      "Date Collection of \"Receiver Name\" is finished\n",
      "Extracting the data for \"Remark from buyer\" header\n",
      "Date Collection of \"Remark from buyer\" is finished\n",
      "Extracting the data for \"Return / Refund Status\" header\n",
      "Date Collection of \"Return / Refund Status\" is finished\n",
      "Extracting the data for \"SKU Reference No.\" header\n",
      "Date Collection of \"SKU Reference No.\" is finished\n",
      "Extracting the data for \"SKU Total Weight\" header\n",
      "Date Collection of \"SKU Total Weight\" is finished\n",
      "Extracting the data for \"Seller Absorbed Coin Cashback\" header\n",
      "Date Collection of \"Seller Absorbed Coin Cashback\" is finished\n",
      "Extracting the data for \"Seller Bundle Discount\" header\n",
      "Date Collection of \"Seller Bundle Discount\" is finished\n",
      "Extracting the data for \"Seller Discount\" header\n",
      "Date Collection of \"Seller Discount\" is finished\n",
      "Extracting the data for \"Seller Rebate\" header\n",
      "Date Collection of \"Seller Rebate\" is finished\n",
      "Extracting the data for \"Seller Voucher\" header\n",
      "Date Collection of \"Seller Voucher\" is finished\n",
      "Extracting the data for \"Service Fee\" header\n",
      "Date Collection of \"Service Fee\" is finished\n",
      "Extracting the data for \"Ship Time\" header\n",
      "Date Collection of \"Ship Time\" is finished\n",
      "Extracting the data for \"Shipment Method\" header\n",
      "Date Collection of \"Shipment Method\" is finished\n",
      "Extracting the data for \"Shipping Option\" header\n",
      "Date Collection of \"Shipping Option\" is finished\n",
      "Extracting the data for \"Shopee Bundle Discount\" header\n",
      "Date Collection of \"Shopee Bundle Discount\" is finished\n",
      "Extracting the data for \"Shopee Coins Offset\" header\n",
      "Date Collection of \"Shopee Coins Offset\" is finished\n",
      "Extracting the data for \"Shopee Rebate\" header\n",
      "Date Collection of \"Shopee Rebate\" is finished\n",
      "Extracting the data for \"Shopee Voucher\" header\n",
      "Date Collection of \"Shopee Voucher\" is finished\n",
      "Extracting the data for \"State\" header\n",
      "Date Collection of \"State\" is finished\n",
      "Extracting the data for \"Total Amount\" header\n",
      "Date Collection of \"Total Amount\" is finished\n",
      "Extracting the data for \"Town\" header\n",
      "Date Collection of \"Town\" is finished\n",
      "Extracting the data for \"Tracking Number*\" header\n",
      "Date Collection of \"Tracking Number*\" is finished\n",
      "Extracting the data for \"Transaction Fee\" header\n",
      "Date Collection of \"Transaction Fee\" is finished\n",
      "Extracting the data for \"Username (Buyer)\" header\n",
      "Date Collection of \"Username (Buyer)\" is finished\n",
      "Extracting the data for \"Variation Name\" header\n",
      "Date Collection of \"Variation Name\" is finished\n",
      "Extracting the data for \"Voucher Code\" header\n",
      "Date Collection of \"Voucher Code\" is finished\n",
      "Extracting the data for \"Zip Code\" header\n",
      "Date Collection of \"Zip Code\" is finished\n"
     ]
    }
   ],
   "source": [
    "info = []\n",
    "header = []\n",
    "for header in mutual_headers:\n",
    "    sub_info = []\n",
    "    print('Extracting the data for \"{}\" header'.format(header))\n",
    "\n",
    "    # Here we are taking the file names in the directory including the address\n",
    "    for item in glb.glob(\"G:\\Google\\Electronic Design\\python_projects\\Insight of Ecommerce Data/2020\\*\"):\n",
    "\n",
    "        data = pd.read_excel(item,\n",
    "                             sheet_name=\"orders\",\n",
    "                             header=None,\n",
    "                             index_col=None)\n",
    "        data_np = data.to_numpy()\n",
    "        loc = np.argwhere(data_np == header)\n",
    "        #if the loc is empty or [0, 0] then loc.any()\"OR function\" will return False\n",
    "        loc2 = loc[0]\n",
    "        # Y is the number of ROWs and X is the number of columns\n",
    "        [y, x] = data_np.shape\n",
    "        # Collect and add the relevant data from the excel file.\n",
    "        sub_info.extend(data_np[1:y, loc2[1]])\n",
    "    # Forming the main data which has a few arrays equal to number of members in List_interest\n",
    "    print('Date Collection of \"{}\" is finished'.format(header))\n",
    "    info.append(sub_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full DATA is unique now and size is \"53\"\n",
      " Process is finished\n"
     ]
    }
   ],
   "source": [
    "remover = True\n",
    "if remover:\n",
    "    info = remove_repeated_data(info)\n",
    "    writer = pd.ExcelWriter('Full_Extracted_Data_Duplicates_removed.xlsx', engine='xlsxwriter')\n",
    "    print('Full DATA is unique now and size is \"{}\"'.format(info.shape[0]))\n",
    "else:\n",
    "    writer = pd.ExcelWriter('Full_Extracted_Data_Original.xlsx', engine='xlsxwriter')\n",
    "    print('The Full Original DATA is \"{}\"'.format(np.shape(info)))\n",
    "\n",
    "# Here we will combine two data and create a dictionary to be saved in Excel file\n",
    "dic = dict(zip(mutual_headers, info))\n",
    "# Change the Numpy format to Pandas format to save in Excel file\n",
    "panda_dic = pd.DataFrame(dic)\n",
    "# Data is the Excel's Sheet name to save the data in.\n",
    "panda_dic.to_excel(writer, 'Data')\n",
    "writer.save()\n",
    "\n",
    "print(' Process is finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
